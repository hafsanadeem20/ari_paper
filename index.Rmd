---
title: The Automated R Instructor
author:
  - name: Sean Kross
    affiliation: Cognitive Science, University of California, San Diego
    address:
    - 9500 Gilman Dr. 
    - La Jolla, CA 92093
    email:  author1@work
  - name: John Muschelli
    affiliation: Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health
    address:
    - 615 N Wolfe Street
    - Baltimore, MD 21231
    email:  jmusche1@jhu.edu
  - name: Jeffrey T. Leek
    affiliation: Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health
    address:
    - 615 N Wolfe Street
    - Baltimore, MD 21231
    email:  jtleek@jhu.edu
abstract: >
  We present the `ari` package for video generation of teaching materials.  The goal of the package is to be able to generate reproducible videos, with the ability to change and update videos seamlessly.  We present an example of generating videos with RMarkdown slide decks with inline comments as the spoken script along with examples using PowerPoint slides or simple images.  We also discusss how these videos can be translated into a number of languages from multiple input formats.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
bibliography: "RJreferences.bib"
---

## Introduction

Videos are a crucial way people learn and pervasive in online education platforms.  Creating videos of a speaker with slides take time, energy, and usually video editing skills.  A large issue with such videos is that updating the materials either requires remaking the entire video or extensive editing and splicing of new segments.  We present \CRANpkg{ari}, the automated R instructor to mitigate these issues by creating reproducible presentations and videos that can be automatically generated.  By using \pkg{ari}, we provide a tools for users to rapidly create and update video content.

```{r echo = FALSE, message=FALSE}
library(rvest)
library(dplyr)
library(text2speech)
library("knitcitations")
cleanbib()
doc = xml2::read_html("https://docs.aws.amazon.com/polly/latest/dg/voicelist.html")
tab = doc %>% 
  html_table()
tab = tab[[2]]
dialects = unique(tab$Language)
languages = sub(", .*", "", dialects)
languages = sub(" .*", "", languages)
languages = languages %>% 
  trimws %>% 
  unique()
```

The premise of the \pkg{ari} package is that you have visual content (e.g. slides, figures) and you want to explain them with words (i.e. a script) in a video.  Voice synthesizer services are available from [Google](https://cloud.google.com/text-to-speech/), [Microsoft](https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/), and [Amazon](https://aws.amazon.com/polly/).  Many of these synthesizers take make use of deep learning methods, such as WaveNet [@van2016wavenet] and have interfaces in R [@googleLanguageR; @mscstts; @aws.polly].  Currently in \pkg{ari}, synthesis of the the audio can be rendered using any of these services through the \CRANpkg{text2speech} [@text2speech].  The default is [Amazon Polly](https://aws.amazon.com/polly/), which has text to speech voice generation in over `r length(languages)` languages, including a total of `r length(dialects)` dialects, implemented in the \CRANpkg{aws.polly} package [@aws.polly].  In addition to multiple languages, the speech generation services provide voices of different genders within the same language.  We present the \pkg{ari} package with reproducible use case examples and the video outputs with different voices in multiple languages.


The \pkg{ari} package relies on the \CRANpkg{tuneR} package for reading and manipulating audio output to combine split audio files and to add pauses to audio files between slides [@tuneR].  Once the audio is generated, it much be spliced with the images to make the video.  Multiple open source tools for video editing and splicing exist.  The `ffmpeg` (http://www.ffmpeg.org/) software is highly powerful, has been thoroughly tested, and has been developed for almost 20 years; \pkg{ari} uses `ffmpeg` to overlay the images over the audio. The output videos have been tested on multiple platforms, including the YouTube and Coursera players.  A default specification is used in \pkg{ari}, such as bitrate, audio and video codecs used, and output video format. GThe numerous additional video specifications can be applied to command-line arguments `ffmpeg` through \pkg{ari}.

With these tools together, we can generate automated videos; we have used it for educational videos.  The spoken scripts for these videos can be stored in plain text, and therefore be version controlled, edited, and updated easily.  If the figures are created in a reproducible framework, such as generated using R code, the entire video can be reproducibly created and automatically updated.  Thus, \pkg{ari} is the Automated R Instructor.  We will provide examples of creating videos based on a slide deck in RMarkdown, a set of images and a script, and discuss how to create slides using a Google Slide deck or PowerPoint presentation.
<!-- `ffmpeg_opts` argument of `ari_stitch`. -->


## Making videos with `ari`

The main workhorse of \pkg{ari} is the `ari_stitch` function.  In this simple example, we assume we already have audio to overlay on some images. The `ari_stitch` function takes the audio and images, and "stitches" them together using `ffmpeg`.  In order to use \pkg{ari}, one must have an `ffmpeg` installation to combine the audio and images, which can be obtained at https://ffmpeg.org/.  In the example below, 2 images are overlaid withe white noise for demonstration.  This example also allows users to check if the output of `ffmpeg` works with a desired video player.

```{r, message = FALSE}
library(tuneR)
library(ari)
result = ari_stitch(
  ari_example(c("mab1.png", "mab2.png")),
  list(noise(), noise()),
  output = "noise.mp4")
isTRUE(result)
outfile = attributes(result)$outfile
```

The output is a logical indicator of success and the path of the output file.  The video for this output can be seen at https://youtu.be/3kgaYf-EV90.  

## Synthesizer authentication

In most cases, however, we do not have audio to overlay on images, but must generate it. 
Though one can generate the spoken audio in many ways, such as fitting a custom deep learning model, we will use the aforementioned services (e.g. Google) as they have direct APIs for use.  The downside of using such services is that users must go through steps to provide authentication, whereas most of these APIs and the associated R packages do not allow for interactive authentication such as OAuth.  

The \pkg{text2speech} package provides a unified interface to these 3 services, and we will focus on Amazon Polly and its authentication requirements.  Polly is authenticated using the  \CRANpkg{aws.signature} package [@aws.signature].  The \pkg{aws.signature} documentation provides options and steps to create the relevant credentials; we have also provided an additional [tutorial](http://seankross.com/2017/05/02/Access-Amazon-Web-Services-in-R.html).  Essentially, the user must sign up for the service and retrieve public and private API keys and put them into their R profile or other areas accesssible to R.  Running `text2speech::tts_auth(service = "amazon")` will indicate if authentication was successful (if using a different service, change the  `service` argument).  NB: The APIs are generally paid services, but many have free tiers or limits, such as Amazon Polly's free tier for the first year (https://aws.amazon.com/polly/pricing/). 


## Making videos with `ari`

After the steps above, videos can be using the `ari_spin` function with a set of images and of text.  This text is the "script" that is spoken over the images to create the output video.  The number of elements in the text need to be equal to the number of images.  Let us take the Mercutio's speech from Shakespeare's Romeo and Juliet [@shakespeare2003romeo] and overlay it on the same images:

```{r, echo = TRUE}
speech =  c(
  "I will now perform part of Mercutio's speech from Shakespeare's Romeo and Juliet.", 
  "O, then, I see Queen Mab hath been with you.
She is the fairies' midwife, and she comes
In shape no bigger than an agate-stone
On the fore-finger of an alderman,
Drawn with a team of little atomies
Athwart men's noses as they lies asleep;")
```
```{r, echo = TRUE, eval = FALSE}
shakespeare_result = ari_spin(
  ari_example(c("mab1.png", "mab2.png")),
  speech, output = "romeo.mp4", voice = "Joanna")
isTRUE(shakespeare_result)
```

```{r, echo = FALSE, eval = TRUE}
output = "romeo.mp4"
if (!file.exists(output)) {
  res = ffmpeg_audio_codecs()
  fdk_enabled = grepl("fdk", res[ res$codec == "aac", "codec_name"])
  if (fdk_enabled) {
    audio_codec = "libfdk_aac"
  } else {
    audio_codec = "aac"
  }
  run_voice = "Joanna"
  ari_spin(
    ari_example(c("mab1.png", "mab2.png")),
    speech, output = output, voice = run_voice,
    service = "amazon",
    audio_codec = audio_codec)
}
```

The speech output can be seen at https://youtu.be/ZCClmUv95iY.  We chose the voice "Joanna" to the the female US-English speaker for the script.  The voices are language-dependent; we can see the available voices for English for Amazon Polly below (from https://docs.aws.amazon.com/polly/latest/dg/SupportedLanguage.html):

```{r, echo = FALSE}
text2speech::tts_voices(service = "amazon") %>% 
  filter(grepl("en", language_code))
```

Though the voice generation is relatively clear, we would not classify the speech as passionate or with a high level of emphasis.  Thus, be believe these videos may be best used for conveying information or education.   We can also generate the video using the voice `Brian`, which is an British English male voice:

```{r, echo = TRUE, eval = FALSE}
gb_result = ari_spin(
  ari_example(c("mab1.png", "mab2.png")),
  speech, output = "romeo_gb.mp4", voice = "Brian")
isTRUE(gb_result)
```

```{r, echo = FALSE, eval = TRUE}
output = "romeo_gb.mp4"
if (!file.exists(output)) {
  res = ffmpeg_audio_codecs()
  fdk_enabled = grepl("fdk", res[ res$codec == "aac", "codec_name"])
  if (fdk_enabled) {
    audio_codec = "libfdk_aac"
  } else {
    audio_codec = "aac"
  }
  run_voice = "Brian"
  ari_spin(
    ari_example(c("mab1.png", "mab2.png")),
    speech, output = output, voice =  run_voice,
    service = "amazon",
    audio_codec = audio_codec)
}
```

The speech output can be seen at https://youtu.be/VuaeRKvs-Y4. The output video format is MP4 by default, but can be any format (aka "muxers") that the `ffmpeg` installation support, see `ari::ffmpeg_muxers`.  Supported codecs can be founded using the functions `ffmpeg_audio_codecs` and `ffmpeg_video_codecs`.  The images and script can be presented in a number of ways, such as a text file and a series of PNG images.  More likely, the images and script will be bundled together, such as a Google Slide deck/Powerpoint presentation with the script in the notes section, or an HTML slide presentation based in RMarkdown, where the script is in the HTML comments.  

#stopped here
--------

For most R users, we believe the most natural setting is that the user has a slide deck using RMarkdown, for example using the \CRANpkg{rmarkdown} or \CRANpkg{xaringan} packages [@rmarkdown; @rmarkdownbook; @xaringan].  The HTML slides are rendered using \CRANpkg{webshot} [@webshot] and the script is located in HTML comments (i.e. between `<!--` and `-->`).  This setup allows for one plain text, version-controllable, integrated document that can reproducibly generate a video.  

Some HTML slides take a bit to render on \pkg{webshot}; for example may be rendered dark gray instead of white.  If you change the `delay` argument in `ari_narrate`, passed to \pkg{webshot}, this can resolve some issues by allowing the page to fully render, but may take a bit longer to run.  Also, the argument `capture_method` allows for the control on how `webshot` is run.  Using the value `vectorized`, webshot is run on the entire slide deck and is faster, but may have some issues.  The value `iterative` runs `webshot` for each slide separately, which can be more robust, but can be slower.  

Users can pass in both the RMarkdown document and the resulting output, or simply the document, and the output will be created using `render` from \pkg{rmarkdown} [@rmarkdown].

```r
# Create a video from an R Markdown file with comments and slides
res = ari_narrate(
  script = ari_example("ari_comments.Rmd"),
  voice = "Kendra",
  capture_method = "iterative")
```





## Accessibility


With respect to accessibility, as \pkg{ari} has the direct script that was spoken, this provides for direct subtitles for those hard of hearing rather than relying on other services, such as YouTube, to provide a speech to text translation.  Though some changes to the script are required for AMazon Polly to correctly pronounce the information, these can be changed using regular expressions in the script, and then passed to `ari_subtitles`.  


## Technical stuff

The \pkg{ari} package relies on [FFmpeg](https://ffmpeg.org/) (>= 3.2.4) to interleave the images and the audio files.


## Future directions

We believe the heavy reliance on an `ffmpeg` installation can be mitigated in the future with advances in the \pkg{av} package.  Though the \pkg{av} package has powerful functionality and is currently porting more from `libav` and therefore `ffmpeg`, it currently does not have the capabilities requried for \pkg{ari}.  Although third party installation from https://ffmpeg.org/ can be burdensome to a user, package managers such as `brew` for OSX and `choco` for Windows provide installations. 

Although we rely on Amazon Polly for voice synthesis, other packages provide voice synthesis, such as \CRANpkg{mscstts} for Microsoft and \CRANpkg{googleLanguageR} for Google.  We aim to harmonize these synthesis options, so that users can choose to create videos with the services that they support or have access to.

Scripts can be automatically translated into other languages with services
like the [Google Translation API](https://cloud.google.com/translate/docs/), which \pkg{googleLanguageR} provides an interface.  Amazon Polly can speak languages other than English. This means you can write a lecture once and generate slides and videos in multiple languages.

We have created a Docker environment (https://github.com/seankross/bologna) with the requirements to create videos using \pkg{ari}.  This Docker image increases the level of reproducibility and can be used to create standalone disk images to create content.


## Examples (FROM README - edit)


These examples make use of the `ari_example()` function. In order to view the
files mentioned here you should use `file.show(ari_example("[file name]"))`.
You can watch an example of a video produced by Ari
[here](https://youtu.be/dcIUu4GCOKU).

```R
library(ari)

# First set up your AWS keys
Sys.setenv("AWS_ACCESS_KEY_ID" = "EA6TDV7ASDE9TL2WI6RJ",
           "AWS_SECRET_ACCESS_KEY" = "OSnwITbMzcAwvHfYDEmk10khb3g82j04Wj8Va4AA",
           "AWS_DEFAULT_REGION" = "us-east-2")

# Create a video from a Markdown file and slides
ari_narrate(
  ari_example("ari_intro_script.md"),
  ari_example("ari_intro.html"),
  voice = "Joey")

# Create a video from an R Markdown file with comments and slides
ari_narrate(
  ari_example("ari_comments.Rmd"),
  ari_example("ari_intro.html"),
  voice = "Kendra")

# Create a video from images and strings
ari_spin(
  ari_example(c("mab1.png", "mab2.png")),
  c("This is a graph.", "This is another graph"),
  voice = "Joanna")
```

### RMarkdown/HTML slide Problems



```r
ari_narrate(
  ari_example("ari_comments.Rmd"),
  ari_example("ari_intro.html"),
  voice = "Kendra",
  delay = 0.5,
  capture_method = "iterative")
```

```{r, include = FALSE, eval = FALSE}
# write.bibtex(file = "RJreferences.bib", append = TRUE)
```

\bibliography{RJreferences}
